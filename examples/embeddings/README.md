# Embeddings

This folder contains examples for getting pretrained embedding vectors.

## What is Word Embedding?

>Word embedding is a technique to map words or phrases from a vocabulary to vectors or real numbers.
>The learned vector representations of words capture  syntactic and semantic word relationships and therefore can be very useful for  tasks like sentence similary, text classifcation, etc.

https://github.com/microsoft/nlp-recipes/blob/master/examples/embeddings/README.md

## Japanese pretrained models

There is a survey article titled "[学習済み日本語word2vecとその評価について](https://blog.hoxo-m.com/entry/2020/02/20/090000)". This article introduces many Japanese pretrained embedding models avaliable and evaluate them.

## Summary

| Notebook                                                  | Environment | Description                                                                                                    |
| --------------------------------------------------------- | ----------- | -------------------------------------------------------------------------------------------------------------- |
| [Word2vec](get_word2vec.py)                               | Local       | Get [word2vec vectors pretrained by Japanese Wikipedia](https://qiita.com/Hironsan/items/513b9f93752ecee9e670) |
| [fastText](get_fasttext.py)                               | Local       | Get [fastText vectors pretrained by Japanese Common Crawl](https://fasttext.cc/docs/en/crawl-vectors.html)     |
| [Download Pre-trained Embeddings](download_embeddings.py) | Local       | Download pre-trained embeddings by [chakin](https://github.com/chakki-works/chakin)                            |
| [Universal Sentence Encoder](get_use.py)                  | Local       | Get [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)           |
